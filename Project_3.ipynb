{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Project_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTpnyUEJrAjE"
      },
      "source": [
        "# **Project 3**, APS1070 Fall 2020\n",
        "#### **PCA [20 marks]**\n",
        "**Deadline: Nov 8, 23:59**\n",
        "\n",
        "**Academic Integrity**\n",
        "\n",
        "This project is individual - it is to be completed on your own. If you have questions, please post your query in the APS1070 Piazza Q&A forums (the answer might be useful to others!).\n",
        "\n",
        "Do not share your code with others, or post your work online. Do not submit code that you have not written yourself. Students suspected of plagiarism on a project, midterm or exam will be referred to the department for formal discipline for breaches of the Student Code of Conduct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUdJ6xw3rJIG"
      },
      "source": [
        "Please fill out the following:\n",
        "\n",
        "\n",
        "*   **Name**:\n",
        "*   **Student number**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwVsZkjMwA5N"
      },
      "source": [
        "We will work on three datasets:\n",
        "* The first Covid-19 dataset reports the number of total cases for different countries at the end of each day. We will use this dataset in **Parts 1-4**.\n",
        "* The second Covid-19 dataset reports the total number of deaths for each country at the end of each day. We use this dataset in **Part 4**. \n",
        "* Finally, we will apply PCA to images using the MNIST dataset of handwritten digits in **Part 5**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BHS-H5SwA5N"
      },
      "source": [
        "# Part 1: Getting started [3 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILjIVdnsXH2u"
      },
      "source": [
        "import pandas as pd\n",
        "cases_raw = pd.read_csv(\n",
        "    filepath_or_buffer='https://raw.githubusercontent.com/aps1070-2019/datasets/master/COVID-OCT_Cases.csv',\n",
        "    index_col=0,\n",
        "    thousands=','\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVOW-vqbwA5N"
      },
      "source": [
        "1. Plot the time-series for the `US`, `China`, ` Canada`, and 7 other countries of your choice. Plot them in separate graphs (subplots), so you can easily see the trend for each country. **[1]**\n",
        "2. Apply `StandardScalar` to the data. Each day should have a `mean` of zero and a `StD` of 1. **[0.5]**\n",
        "3. Plot the standardized time-series for the same countries as you chose in `Step 1`. **[0.5]**\n",
        "4. Discuss the trends in the standardized time-series for the `US`, `Canada`, and `China`. Why does it man if the curve goes up or down? What do negative and positive values mean? **[1]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9IN_qKNERbj"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4xcf_DpwA5N"
      },
      "source": [
        "# Part 2: Applying PCA [3 Marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch7Ly5XQwA5N"
      },
      "source": [
        "1. Compute the covariance matrix of the dataframe. *Hint: The dimensions of your covariance matrix should be (265, 265).* **[0.5]**\n",
        "2. Compute eigenvalues and eigenvectors using `np.linalg.eigh`. **[0.5]**\n",
        "3. Show the effectiveness of your principal components in covering the variance of the dataset with a `scree plot`. **[0.5]**\n",
        "4. How many PCs do you need to cover 99\\% of the dataset's variance? **[0.5]**\n",
        "5. Show the first 20 principal components (Eigenvectors) plotted as a time series (20 subplots). **[0.5]**\n",
        "6. Compare the first few PCs with the rest of them. Do you see any difference in their trend? **[0.5]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2gQgts5ESF6"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmndIgPVwA5O"
      },
      "source": [
        "# Part 3: Data reconstruction [6 Marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvtD5MnGwA5O"
      },
      "source": [
        "Create a function that:\n",
        "\n",
        "*   Accepts a country name as an input.\n",
        "*   Plots 4 figures:\n",
        "1.   The standardized time-series for the specified country. **[0.5]**\n",
        "2.   The incremental reconstruction of the standardized time-series for the specified country in a single plot with the 10 first principal components (10 curves: the first curve a reconstruction with PC1, the second with PC1+PC2, the third with PC1+PC2+PC3, etc.). **[2]**\n",
        "3.   The residual error of reconstruction for each of the 10 reconstructions with respect to the standardized time-series. **[1]**\n",
        "4.   The RMSE of reconstruction as a function of number of included components. **[1]**\n",
        "\n",
        "*   Prints how many PCs are needed so that the RMSE would be less than $1$, $0.1$ and $0.01$. **[1]**\n",
        "\n",
        "Test your function using the `US`, `Canada`, and `China` as inputs. **[0.5]**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKXBlzSOESoE"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WELppYZjL6Bu"
      },
      "source": [
        "# Part 4: Time-series analysis on death cases [3 Marks]\n",
        "Here we'll use another dataset for total deaths caused by COVID-19 for different countries. \n",
        "\n",
        "Call the function that you designed in *Part 3* with this new dataset. To do that you'll first need to preprocess the data (follow the steps we have in Parts 1 and 2) **[1]** and prepare the data for calling the Part 3 function **[1]**. Test your function with the new dataset, with the `US`, `Canada`, and `China`. **[1]**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvl7Tvy_M-v-"
      },
      "source": [
        "import pandas as pd\n",
        "death_raw = pd.read_csv(\n",
        "    filepath_or_buffer='https://raw.githubusercontent.com/aps1070-2019/datasets/master/COVID-OCT_Dead.csv',\n",
        "    index_col=0,\n",
        "    thousands=','\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyDCvZoZNcRM"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lysglXu_BGV_"
      },
      "source": [
        "# Part 5: MNIST dataset [5 Marks]\n",
        "MNIST is a dataset for hand-written digits recognition. \n",
        "Each image in MNIST has 28x28 pixels which can be represented in the form of an array with 784 elements. In this part we are going to use PCA to compress these images. The $x$ matrix below has 1000 images.\n",
        "* Compute the covariance of the dataframe and perform eigendecomposition.  **[1]**\n",
        "* Plot the first 10 eigenvectors. What trends do you observe? **[1]**\n",
        "* Create a function that requires an argument $n$ and plots a random image from the dataset, as well as $n$ plots of its incremental reconstruction (a first reconstruction with PC1, the second with PC1+PC2, the third with PC1+PC2+PC3, etc.). **[1]**\n",
        "* Run your function a few times, and determine how many components are required so that we (humans!) can identify the digits. **[1]**\n",
        "* With that many components, what would be the compression ratio for a dataset with 1000 images? *Hint: compare the dataset size with a case where we only use principal components and projections* **[1]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOXm6yPqETJc"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "x = mnist.data[0:1000]\n",
        "y = mnist.target[0:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5tOkaA7DTbW"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r13-BG5Scfbr"
      },
      "source": [
        "x.shape\n",
        "import matplotlib.pyplot as plt \n",
        "plt.gray() \n",
        "plt.imshow(x[2].reshape(28,28)) \n",
        "print (\"Label is:\", y[2])\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxQLdDC7SFxL"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rKZXBodwA5P"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA-T9S4RwA5P"
      },
      "source": [
        "Understanding PCA and SVD:\n",
        "1. https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8\n",
        "2. https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca\n",
        "3. https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
        "4. https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/\n",
        "\n",
        "PCA:\n",
        "1. Snippets from: https://plot.ly/ipython-notebooks/principal-component-analysis/\n",
        "2. https://www.value-at-risk.net/principal-component-analysis/\n",
        "\n",
        "Covid Data:\n",
        "1. https://www.worldometers.info/coronavirus/\n",
        "2. https://datahub.io/core/covid-19#resource-time-series-19-covid-combined\n",
        "\n",
        "\n"
      ]
    }
  ]
}